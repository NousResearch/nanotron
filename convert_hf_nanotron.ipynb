{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.testing import assert_close\n",
    "\n",
    "import os\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"0.0.0.0\"\n",
    "os.environ[\"MASTER_PORT\"] = \"6000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solergib/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "PATH_TO_LLAMA = \"/mloscratch/homes/solergib/models/Meta-Llama-3-8B-Instruct\"\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(PATH_TO_LLAMA, torch_dtype=dtype, attn_implementation=\"flash_attention_2\").to(device)\n",
    "# print(hf_model)\n",
    "# print(hf_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig\n",
    "hf_config = LlamaConfig.from_pretrained(PATH_TO_LLAMA)\n",
    "print(hf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.config import ParallelismArgs\n",
    "from nanotron.parallel import ParallelContext\n",
    "from nanotron.parallel.pipeline_parallel.engine import AllForwardAllBackwardPipelineEngine\n",
    "from nanotron.parallel.tensor_parallel.nn import TensorParallelLinearMode\n",
    "\n",
    "DP = 1\n",
    "PP = 1\n",
    "TP = 1\n",
    "\n",
    "parallel_config = ParallelismArgs(\n",
    "    dp=DP,\n",
    "    pp=PP,\n",
    "    tp=TP,\n",
    "    pp_engine=AllForwardAllBackwardPipelineEngine(),\n",
    "    tp_mode=TensorParallelLinearMode.ALL_REDUCE,\n",
    "    tp_linear_async_communication=False,\n",
    ")\n",
    "assert (\n",
    "    parallel_config.tp_mode == TensorParallelLinearMode.ALL_REDUCE\n",
    "    and parallel_config.tp_linear_async_communication is False\n",
    ")\n",
    "\n",
    "parallel_context = ParallelContext(\n",
    "    data_parallel_size=parallel_config.dp,\n",
    "    pipeline_parallel_size=parallel_config.pp,\n",
    "    tensor_parallel_size=parallel_config.tp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.config.models_config import LlamaConfig as LlamaConfigNanotron\n",
    "\n",
    "nanotron_config = LlamaConfigNanotron(\n",
    "    bos_token_id=hf_config.bos_token_id,\n",
    "    eos_token_id=hf_config.eos_token_id,\n",
    "    hidden_act=hf_config.hidden_act,\n",
    "    hidden_size=hf_config.hidden_size,\n",
    "    initializer_range=hf_config.initializer_range,\n",
    "    intermediate_size=hf_config.intermediate_size,\n",
    "    is_llama_config=True,\n",
    "    max_position_embeddings=hf_config.max_position_embeddings,\n",
    "    num_attention_heads=hf_config.num_attention_heads,\n",
    "    num_hidden_layers=hf_config.num_hidden_layers,\n",
    "    num_key_value_heads=hf_config.num_key_value_heads,\n",
    "    pad_token_id=None,\n",
    "    pretraining_tp=hf_config.pretraining_tp,\n",
    "    rms_norm_eps=hf_config.rms_norm_eps,\n",
    "    rope_scaling=hf_config.rope_scaling,\n",
    "    rope_theta=hf_config.rope_theta,\n",
    "    rope_interleaved=False,\n",
    "    tie_word_embeddings=hf_config.tie_word_embeddings,\n",
    "    use_cache=hf_config.use_cache,\n",
    "    vocab_size=hf_config.vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.models.llama_sft import LlamaForSFT\n",
    "from nanotron.models import build_model\n",
    "\n",
    "nanotron_model = build_model(\n",
    "        model_builder=lambda: LlamaForSFT(\n",
    "            config=nanotron_config,\n",
    "            parallel_context=parallel_context,\n",
    "            parallel_config=parallel_config,\n",
    "            random_states=None,\n",
    "        ),\n",
    "        parallel_context=parallel_context,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    ")\n",
    "# print(nanotron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.trainer import mark_tied_parameters\n",
    "\n",
    "mark_tied_parameters(model=nanotron_model, parallel_context=parallel_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardedInfo(global_ranks=(0,), local_global_slices_pairs=(SlicesPair(local_slices=(slice(None, None, None), slice(None, None, None)), global_slices=(slice(0, 128256, None), slice(None, None, None))),), unsharded_shape=(128256, 4096))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nanotron_model.model.token_position_embeddings.pp_block.token_embedding.weight.get_sharded_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nanotron_model.model.token_position_embeddings.pp_block.token_embedding.weight.is_tied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final script\n",
    "# TODO Añadir variables de TP para splitear los parametros de las layers de HF\n",
    "# TODO Cargar modelo HF en cpu y copiar desde ahi\n",
    "\n",
    "\n",
    "# Token embeddings\n",
    "assert nanotron_model.model.token_position_embeddings.pp_block.token_embedding.weight.shape == hf_model.model.embed_tokens.weight.shape\n",
    "\n",
    "with torch.no_grad():\n",
    "    nanotron_model.model.token_position_embeddings.pp_block.token_embedding.weight.copy_(hf_model.model.embed_tokens.weight)#  = hf_model.model.embed_tokens.weight.data\n",
    "\n",
    "# Decoder layers\n",
    "for i in range(nanotron_config.num_hidden_layers):\n",
    "    # Input layer norm\n",
    "    assert hf_model.model.layers[i].input_layernorm.weight.shape == nanotron_model.model.decoder[i].pp_block.input_layernorm.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.input_layernorm.weight.copy_(hf_model.model.layers[i].input_layernorm.weight)#  = hf_model.model.layers[i].input_layernorm.weight\n",
    "    # Self attn\n",
    "    ## QKV\n",
    "    tmp_qkv_proj = torch.cat([\n",
    "        hf_model.model.layers[i].self_attn.q_proj.weight,\n",
    "        hf_model.model.layers[i].self_attn.k_proj.weight,\n",
    "        hf_model.model.layers[i].self_attn.v_proj.weight\n",
    "    ], dim = 0) \n",
    "    assert tmp_qkv_proj.shape == nanotron_model.model.decoder[i].pp_block.attn.qkv_proj.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.attn.qkv_proj.weight.copy_(tmp_qkv_proj)#  = tmp_qkv_proj # torch.nn.Parameter(tmp_qkv_proj)\n",
    "    \n",
    "    ## O\n",
    "    assert hf_model.model.layers[i].self_attn.o_proj.weight.shape == nanotron_model.model.decoder[i].pp_block.attn.o_proj.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.attn.o_proj.weight.copy_(hf_model.model.layers[i].self_attn.o_proj.weight)#  = hf_model.model.layers[i].self_attn.o_proj.weight\n",
    "    # MLP\n",
    "    ## Gate Up Proj\n",
    "    tmp_gate_up_proj = torch.cat([\n",
    "        hf_model.model.layers[i].mlp.gate_proj.weight,\n",
    "        hf_model.model.layers[i].mlp.up_proj.weight,\n",
    "    ], dim = 0)\n",
    "\n",
    "    assert tmp_gate_up_proj.shape == nanotron_model.model.decoder[i].pp_block.mlp.gate_up_proj.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.mlp.gate_up_proj.weight.copy_(tmp_gate_up_proj)#  = tmp_gate_up_proj\n",
    "    ## Down Proj\n",
    "    assert hf_model.model.layers[i].mlp.down_proj.weight.shape == nanotron_model.model.decoder[i].pp_block.mlp.down_proj.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.mlp.down_proj.weight.copy_(hf_model.model.layers[i].mlp.down_proj.weight)#  = hf_model.model.layers[i].mlp.down_proj.weight\n",
    "\n",
    "\n",
    "    # Post attn layer norm\n",
    "    assert hf_model.model.layers[i].post_attention_layernorm.weight.shape == nanotron_model.model.decoder[i].pp_block.post_attention_layernorm.weight.shape\n",
    "    with torch.no_grad():\n",
    "        nanotron_model.model.decoder[i].pp_block.post_attention_layernorm.weight.copy_(hf_model.model.layers[i].post_attention_layernorm.weight)#  = hf_model.model.layers[i].post_attention_layernorm.weight\n",
    "    \n",
    "# Last layer norm\n",
    "assert nanotron_model.model.final_layer_norm.pp_block.weight.shape == hf_model.model.norm.weight.shape\n",
    "with torch.no_grad():\n",
    "    nanotron_model.model.final_layer_norm.pp_block.weight.copy_(hf_model.model.norm.weight)#  = hf_model.model.norm.weight\n",
    "# LM_Head\n",
    "assert nanotron_model.model.lm_head.pp_block.weight.shape == hf_model.lm_head.weight.shape\n",
    "with torch.no_grad():\n",
    "    nanotron_model.model.lm_head.pp_block.weight.copy_(hf_model.lm_head.weight)# = hf_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.data.chat_dataset import ChatDataset\n",
    "from nanotron.data.dataloader_builder import build_chat_dataloader\n",
    "\n",
    "train_dataset = ChatDataset(\n",
    "    dataset_path=\"Open-Orca/SlimOrca\",\n",
    "    tokenizer_name_or_path=PATH_TO_LLAMA,\n",
    "    sequence_length=2048,\n",
    "    train_on_completions_only=True,\n",
    "    remove_cross_attention=True,\n",
    "    split=\"train\",\n",
    "    conversation_column_name=\"conversations\",\n",
    "    dp_rank=parallel_context.dp_pg.rank(),\n",
    "    dp_ranks_size=parallel_context.dp_pg.size(),\n",
    ")\n",
    "\n",
    "# Prepare dataloader\n",
    "train_dataloader = build_chat_dataloader(\n",
    "    dataset=train_dataset,\n",
    "    sequence_length=2048,\n",
    "    parallel_context=parallel_context,\n",
    "    input_pp_rank=0,\n",
    "    output_pp_rank=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009]], dtype=torch.int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][:, -150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,  26380,  ...,     13, 128009, 128001]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][:, :-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nanotron_model.eval()\n",
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_nanotron = nanotron_model.model(input_ids=batch[\"input_ids\"][:, :-150].cuda(), position_ids = batch[\"position_ids\"][:, :-150].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n",
      "PEPEPEPEPE\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_hf = hf_model(input_ids=batch[\"input_ids\"][:, :-150].cuda(), position_ids = batch[\"position_ids\"][:, :-150].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 243083431 / 243429888 (99.9%)\nGreatest absolute difference: 46.65625 at index (0, 1125, 22) (up to 1e-05 allowed)\nGreatest relative difference: 74448896.0 at index (0, 715, 31230) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_close\n\u001b[0;32m----> 3\u001b[0m \u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_nanotron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/testing/_comparison.py:1520\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1498\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1499\u001b[0m     actual,\n\u001b[1;32m   1500\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1516\u001b[0m )\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 243083431 / 243429888 (99.9%)\nGreatest absolute difference: 46.65625 at index (0, 1125, 22) (up to 1e-05 allowed)\nGreatest relative difference: 74448896.0 at index (0, 715, 31230) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "from torch.testing import assert_close\n",
    "\n",
    "assert_close(output_hf.logits, output_nanotron.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF Model] Next token: 11415, probability: 0.10412170737981796\n",
      "[HF Model] Next token: 1523, probability: 0.04918361455202103\n",
      "[HF Model] Next token: 47032, probability: 0.043404385447502136\n",
      "[HF Model] Next token: 72514, probability: 0.03830423951148987\n",
      "[HF Model] Next token: 3493, probability: 0.03830423951148987\n",
      "[HF Model] Next token: 10477, probability: 0.03830423951148987\n",
      "[HF Model] Next token: 16805, probability: 0.03175532445311546\n",
      "[HF Model] Next token: 10552, probability: 0.026326090097427368\n",
      "[HF Model] Next token: 7664, probability: 0.021825095638632774\n",
      "[HF Model] Next token: 3041, probability: 0.018093638122081757\n"
     ]
    }
   ],
   "source": [
    "predicted_token = 34\n",
    "\n",
    "next_tokens_hf = torch.softmax(output_hf.logits[0, predicted_token, :], -1)\n",
    "hf_topk_next_tokens= torch.topk(next_tokens_hf, 10)\n",
    "\n",
    "\n",
    "print(*[f\"[HF Model] Next token: {idx.item()}, probability: {prob}\" for idx, prob in zip(hf_topk_next_tokens.indices, hf_topk_next_tokens.values)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nanotron Model] Next token: 220, probability: 0.0804644376039505\n",
      "[Nanotron Model] Next token: 994, probability: 0.029601214453577995\n",
      "[Nanotron Model] Next token: 3639, probability: 0.02612297795712948\n",
      "[Nanotron Model] Next token: 656, probability: 0.024540266022086143\n",
      "[Nanotron Model] Next token: 279, probability: 0.024540266022086143\n",
      "[Nanotron Model] Next token: 3277, probability: 0.021656708791851997\n",
      "[Nanotron Model] Next token: 264, probability: 0.013982621021568775\n",
      "[Nanotron Model] Next token: 1148, probability: 0.01022990420460701\n",
      "[Nanotron Model] Next token: 507, probability: 0.01022990420460701\n",
      "[Nanotron Model] Next token: 323, probability: 0.01022990420460701\n"
     ]
    }
   ],
   "source": [
    "next_tokens_nanotron = torch.softmax(output_nanotron.transpose(0,1)[0, predicted_token, :], -1)\n",
    "nanotron_topk_next_tokens= torch.topk(next_tokens_nanotron, 10)\n",
    "\n",
    "\n",
    "print(*[f\"[Nanotron Model] Next token: {idx.item()}, probability: {prob}\" for idx, prob in zip(nanotron_topk_next_tokens.indices, nanotron_topk_next_tokens.values)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Nanotron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.parallel.parameters import sanity_check\n",
    "\n",
    "sanity_check(root_module=nanotron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving weights: 100%|██████████| 195/195 [00:41<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from nanotron.serialize import save_meta, save_weights, TrainingMetadata\n",
    "from nanotron.serialize.metadata import DataStageMetadata\n",
    "\n",
    "out_path = \"/mloscratch/homes/solergib/converter/nanotron/n_c/first/\"\n",
    "out_path = Path(out_path)\n",
    "\n",
    "save_weights(model=nanotron_model, parallel_context=parallel_context, root_folder=out_path)\n",
    "\n",
    "training_metadata = TrainingMetadata(last_train_step=0, consumed_train_samples=0, data_stages=[DataStageMetadata(name=\"Empty\", consumed_train_samples=0, start_training_step=0)])\n",
    "\n",
    "save_meta(root_folder=out_path, parallel_context=parallel_context, training_metadata=training_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving config ...\n",
      "Saving model config ...\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import yaml\n",
    "from nanotron.config import GeneralArgs, ModelArgs, TokenizerArgs, Config\n",
    "from nanotron.config.models_config import ExistingCheckpointInit\n",
    "from dataclasses import asdict\n",
    "\n",
    "with open(out_path / \"config.yaml\", \"w\") as f:\n",
    "    config = Config(\n",
    "        general=GeneralArgs(project=\"conversion\", run=\"Llama3-8B\"),\n",
    "        parallelism=parallel_config,\n",
    "        model=ModelArgs(\n",
    "            init_method=ExistingCheckpointInit(out_path),\n",
    "            model_config=nanotron_config,\n",
    "        ),\n",
    "        tokenizer=TokenizerArgs(PATH_TO_LLAMA),\n",
    "    )\n",
    "    print(\"Saving config ...\")\n",
    "    yaml.dump(config.as_dict(), f)\n",
    "\n",
    "with open(out_path / \"model_config.json\", \"w\") as f:\n",
    "    print(\"Saving model config ...\")\n",
    "    json.dump(asdict(nanotron_config), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mloscratch/homes/solergib/SFT/transformers/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[27, 22,  0, 97, 13, 49, 56, 35, 70, 91, 38, 30, 26, 94, 68, 46, 89, 32,\n",
      "         70, 85, 50, 67, 70, 86, 66, 82, 18, 72, 27, 37, 91, 27, 60, 57, 23, 93,\n",
      "         10, 80, 82, 26, 13, 50, 12, 68, 63, 85, 55,  1,  3, 61, 37, 70, 12, 97,\n",
      "          1, 59, 90, 45, 74, 62, 66, 54, 94, 18, 54, 89, 49,  3, 66, 55]],\n",
      "       device='cuda:0'), 'position_ids': tensor([[0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 0, 1, 2,\n",
      "         3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
      "         6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mloscratch/homes/solergib/SFT/transformers\")\n",
    "\n",
    "import torch\n",
    "from t_tests.models.llama.test_modeling_llama import LlamaModelTester\n",
    "\n",
    "lmt = LlamaModelTester(parent=None)\n",
    "\n",
    "_, inputs_dict = lmt.prepare_config_and_inputs_for_common()\n",
    "dummy_attention_mask = inputs_dict[\"attention_mask\"]\n",
    "inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = 0\n",
    "\n",
    "padfree_inputs_dict = {\n",
    "    k: v[dummy_attention_mask.bool()].unsqueeze(0)\n",
    "    for k, v in inputs_dict.items()\n",
    "    if not k == \"attention_mask\"\n",
    "}\n",
    "\n",
    "padfree_inputs_dict[\"position_ids\"] = (\n",
    "    torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n",
    "    .long()\n",
    "    .unsqueeze(0)\n",
    "    .to(\"cuda\")\n",
    ")\n",
    "\n",
    "print(padfree_inputs_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
